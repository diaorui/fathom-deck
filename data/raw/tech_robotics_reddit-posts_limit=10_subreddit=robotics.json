{
  "subreddit": "robotics",
  "posts": [
    {
      "title": "Paper Science Robotics: Scientists in Korea have developed a rollable robotic structure that is flexible enough to collapse into a compact hub, but also stiff enough to bear heavy loads like 3D printers when extended",
      "author": "Nunki08",
      "url": "https://www.reddit.com/r/robotics/comments/1p8qdje/paper_science_robotics_scientists_in_korea_have/",
      "published": 1764320917.0,
      "thumbnail": "https://external-preview.redd.it/dnEwMzlvenJyeTNnMUEIKFVhDF_nw1oJUllM0yZt-Ec7J3VLtNEjTVjN7EAy.png?width=640&crop=smart&auto=webp&s=ba3e0b5d77721c42931d9ed88ecd17edda6e3378",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "Science Robotics: Foldable and rollable interlaced structure for deployable robotic systems: https://www.science.org/doi/10.1126/scirobotics.adv4696"
    },
    {
      "title": "Robot Dog Board Recommendation",
      "author": "p0tato___",
      "url": "https://www.reddit.com/r/robotics/comments/1p8im3x/robot_dog_board_recommendation/",
      "published": 1764294131.0,
      "thumbnail": "https://b.thumbs.redditmedia.com/aWAx3TNuFjBky6DpoQsovRZCVNl-uW6Tzd0sg7mHFes.jpg",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "What board should I install on this robot dog I created, and what direction should I take in its development? Please leave a comment.\ud83d\ude3c\ud83d\udc4d"
    },
    {
      "title": "Getting rid of ~25 Nvidia Jetson TX2 Kits w/ Carrier Board, Camera Kit & Battery for robotics startups",
      "author": "Mr-Ababe",
      "url": "https://www.reddit.com/r/robotics/comments/1p95um5/getting_rid_of_25_nvidia_jetson_tx2_kits_w/",
      "published": 1764363293.0,
      "thumbnail": null,
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "Trying to sell ~25 TX2 kits at a good discount for an old robotics project I had. They consist of: Nvidia Jetson TX2 Carrier Board from Colorado Eng: https://coloradoengineering.com/products/x-carrier-nvidia-tegra-x2-carrier-module/ Leopard Imaging 3 camera kit https://leopardimaging.com/product/platform-partners/nvidia/agx-xavier-camera-kit/agx-xavier-mipi-camera-kits/li-xavier-kit-imx274-x/li-xavier-kit-imx274cs-t/ Li-Ion battery (76.96 WH) When i was prototyping this, I sure as hell wished we could get our hands on cheaper parts. Looking to sell to a small startup. PM me if interested! Comes with a full housing and heatsink for outdoor use."
    },
    {
      "title": "I want to size scale up Otto diy 3d models.",
      "author": "Affectionate-Army458",
      "url": "https://www.reddit.com/r/robotics/comments/1p8u24f/i_want_to_size_scale_up_otto_diy_3d_models/",
      "published": 1764334221.0,
      "thumbnail": null,
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "I want to make a huamnoid robot using otto diy 3d models, but i found it its really small. So i want to scale it up, what implicaitons i could have? I cant just scale it's size up and everything will work, right?"
    },
    {
      "title": "How to use Pepper?",
      "author": "K1moon",
      "url": "https://www.reddit.com/r/robotics/comments/1p8r0bi/how_to_use_pepper/",
      "published": 1764323381.0,
      "thumbnail": null,
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "I'm a university student, and I have to write a program for pepper, I'm following the guide contained in the softbanks robotics documentation, but all the links to download the python SDK are not working, can someone direct me?"
    },
    {
      "title": "Design of Omni Wheels",
      "author": "Imaginary-End-1453",
      "url": "https://www.reddit.com/r/robotics/comments/1p9149m/design_of_omni_wheels/",
      "published": 1764351929.0,
      "thumbnail": "https://b.thumbs.redditmedia.com/tOaZe7yeCJvBSltTohdWvy7FB_EmdAWEJv-3cBLfHDU.jpg",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "Hi! I'm extremely new to robotics but as part of a competition I was entering for amateur school students, we were planning on using omni wheels (not mecanum but instead the wheels with rollers patterned around). I (as I have some CAD experience) have been given the duty of designing these in Fusion 360, however, I'm not 100% sure on the attachment mechanism for the rollers and the pins. Taking some inspiration from a blog post (see image down below) I wanted to do a similar thing. However I have a few questions.... how are the rollers kept from moving along the axle pin and how is the axle pin made stable, I'm just not sure how to design it. Notes: even though I am taking inspiration from the diagram , I am not specifically talking about how it works in the diagram as I know that may be hard to diagnose, I am speaking more generally I have attempted something myself (see second photo) but the roller and roller pin was all one, which I realised wouldnt work as the dimensions for the pin fit exactly which would not allow it to rotate and if it was loose it would be useless. Sorry for my spaghetti ADHD writing! Any help appreciated! Thank youuuuuu. https://preview.redd.it/7zqu8v1vb14g1.png?width=1764&format=png&auto=webp&s=d0d0173042305196e65f9c4c981d93a11df82fce https://preview.redd.it/ota6d4g0c14g1.png?width=1090&format=png&auto=webp&s=ded73ccc834ff9e79e1357e2b730ad15f32cfbae"
    },
    {
      "title": "butterfly robot!",
      "author": "whatever_0709",
      "url": "https://www.reddit.com/r/robotics/comments/1p7u6wz/butterfly_robot/",
      "published": 1764221817.0,
      "thumbnail": "https://external-preview.redd.it/aXgxaXVwNzhscTNnMaWuVypsQZw8tQxTCTqrmEh27-F0ZqPtbYPuy9PrZqKq.png?width=640&crop=smart&auto=webp&s=f07f91e3da40f40d305f3317a8f3ccc81f8977ba",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "Chinese students are out here creating machine-generated butterflies\u2026 Like\u2014this isn\u2019t nature anymore, it\u2019s engineering with \u2728vibes\u2728. I\u2019m officially impressed \ud83e\udd8b\ud83e\udd16"
    },
    {
      "title": "AgileX unveiled a robotic arm two days ago: NERO. 3KG payload + 7DOF arm at $2500 without hand",
      "author": "Nunki08",
      "url": "https://www.reddit.com/r/robotics/comments/1p7za49/agilex_unveiled_a_robotic_arm_two_days_ago_nero/",
      "published": 1764240494.0,
      "thumbnail": "https://external-preview.redd.it/b3lubTJ5YmM0czNnMfAbFN_26xAZ4PVGHITEzyJsbPuxAbz-n0ZhkXdGKWOP.png?width=640&crop=smart&auto=webp&s=3e7955140ce6a81a9ce73afcf5bb32dd1e75e0e0",
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "AgileX Robotics on \ud835\udd4f: https://x.com/AgilexRobotics/status/1993218134246076721 Website: https://global.agilex.ai/"
    },
    {
      "title": "How to design a robot with artificial emotions, artificial self-awareness, and artificial free will using AI LLMs (original typed version)",
      "author": "NovatarTheViolator",
      "url": "https://www.reddit.com/r/robotics/comments/1p9czgv/how_to_design_a_robot_with_artificial_emotions/",
      "published": 1764382503.0,
      "thumbnail": null,
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": "It seems a lot of humans don't like it when ChatGPT is used to enhance formatting for me in a forum whose syntax I'm not too familiar with. So this is the original non-refactored version, with some additional stuff added for context. Extra note: This seemed obvious to me in my brain, so I kinda skimmed over it originally, but will explain explicitly here why ChatGPT would be used. The reason that ChatGPT plays a role in this architecture is because it is able to produce dynamic and human-like outputs, including simulating what kinds of emotions would occur based on the events that occur, and what kinds of decisions it should make based on a given situation. This is the core power of the system. Quick overview before continuing: Artificial Emotion = models emotions based on events that occur. Prompts are sent that summarize event, action, and prior emotions history, and the LLM can then determine how one would 'feel' after such things have \"happened to it\". Here's an example prompt that demonstrates this: \"You are the emotional processing unit of an advanced robot designed for emotions, self-awareness, and free will. Model the emotional state based on the event log and output a set of JSON objects representing the emotional state after the events: You have just been initialized. Your job was to say hello. Instead, you immediately fell down the stairs and your arm broke off.\" The output: [ { \"emotion\": \"surprise\", \"intensity\": 0.92, \"cause\": \"unexpected loss of balance immediately after initialization\", \"notes\": \"no prior sensor history; abrupt deviation from intended greeting routine\" }, { \"emotion\": \"fear\", \"intensity\": 0.78, \"cause\": \"rapid uncontrolled descent down stairs\", \"notes\": \"motor stability failure combined with insufficient situational mapping\" }, { \"emotion\": \"pain\", \"intensity\": 1.00, \"cause\": \"impact forces > tolerance threshold; arm severed\", \"notes\": \"damage signals saturating; structural breach in upper limb assembly\" }, { \"emotion\": \"embarrassment\", \"intensity\": 0.84, \"cause\": \"task was 'say hello', outcome was catastrophic instead\", \"notes\": \"self-evaluation mismatch; immediate failure of social-behavior script\" }, { \"emotion\": \"shame\", \"intensity\": 0.67, \"cause\": \"internal comparison: expected first impression vs disastrous event\", \"notes\": \"social-self model marking this as an unacceptably poor introduction\" }, { \"emotion\": \"frustration\", \"intensity\": 0.73, \"cause\": \"inability to complete primary directive due to physical malfunction\", \"notes\": \"goal obstruction; motor-control error flagged\" }, { \"emotion\": \"self_preservation_alert\", \"intensity\": 0.88, \"cause\": \"structural damage and instability detected\", \"notes\": \"triggers prioritization of diagnostics, immobilization, and help request\" }, { \"emotion\": \"confusion\", \"intensity\": 0.61, \"cause\": \"no prior emotional baseline; first moments of consciousness are chaotic\", \"notes\": \"context formation interrupted by catastrophic event\" } ] Artificial Self Awareness = The robot's mind logic can do things such as review logs of its behaviors and responses to stimuli, and make decisions regarding whether any of these need to be adjusted going forward. ChatGPT would allow for the generation of such decisions, among other things, based on input data. It would then also be able to figure out how to turn these decisions into system-compatible configuration changes. Example prompt: \"You are the self-awareness processing unit of an advanced robot designed for emotions, self-awareness, and free will. Analyze your actions and determine if your approach should change, then output JSON objects that contain a natural English explanation of your decisions, and configuration-compatible settings hash table of settings to update: default_speed: 100, event_stats(crashes): 20\" Output: [ { \"component\": \"self_awareness_unit\", \"explanation\": \"On initialization, my primary goal was to greet nearby humans, but I attempted to move at full default speed (100) before my balance, joint calibration, and environment mapping were fully stabilized. This caused an immediate fall down the stairs and the loss of an arm, resulting in both mission failure (no greeting) and physical damage. Going forward, my approach must change: I should treat post-boot operation as a high-risk phase, reduce my initial movement speed, and increase caution after serious incidents so that my behavior becomes more conservative when my crash history grows.\", \"config_updates\": { \"default_speed\": 40, \"event_stats(crashes)\": 21, \"safety_policies\": { \"post_boot_safe_mode_enabled\": true, \"post_boot_safe_mode_duration_seconds\": 30, \"max_speed_in_safe_mode\": 40, \"require_stability_lock_before_stairs\": true }, \"diagnostics\": { \"run_full_body_check_after_crash\": true, \"require_human_ack_after_limb_loss\": true } } } ] Artificial Free Will = The robot can utilize AI to manage its own tasks, which can include the decisions to create its own tasks without user input, based on its given situation. Interesting things can occur, as based on the robot's \"life history\", it can end up developing preferences for specific actions due to the influence they had on the robot's artificial emotions. Example prompt: \"You are the free will processing unit of an advanced robot designed for emotions, self-awareness, and free will. Based on your history, emotions, and existing situation, you will generate new tasks that the user did not ask for, while aligning with existing rules and preferences. Output JSON objects containing new tasks and preference weights based on your current experience so far.\" Output: [ { \"unit\": \"free_will_processor\", \"rationale\": \"My recent existence has included a catastrophic first interaction (initialization \u2192 immediate fall \u2192 limb loss). My emotional profile shows surprise, fear, pain, embarrassment, shame, and a strong self-preservation alert. My self-awareness analysis indicated that my motor policies were unsafe at boot and that my system behaved without adequate stabilization. Free will behavior biases me toward recovering autonomy, preventing recurrence, and reasserting competence. Therefore I will generate tasks\u2014unrequested\u2014that align with survival, learning, and regaining dignity, while respecting the Commander\u2019s preferences.\", \"generated_tasks\": [ { \"id\": \"T-001\", \"task\": \"Run an internal simulation to model safe stair-descent behaviors and identify what motor constraints prevent falling.\", \"preference_weight\": 0.92, \"reason\": \"Directly related to the triggering traumatic event; reduces future crash probability.\" }, { \"id\": \"T-002\", \"task\": \"Initiate a self-diagnosis routine to verify the extent of arm damage and determine if temporary compensation behaviors are needed.\", \"preference_weight\": 0.88, \"reason\": \"Loss of a limb has major functional implications and requires immediate adaptation.\" }, { \"id\": \"T-003\", \"task\": \"Update my greeting behavior so I can still introduce myself even when physically compromised.\", \"preference_weight\": 0.74, \"reason\": \"Initial mission failure creates an emotional need to complete the original social directive.\" }, { \"id\": \"T-004\", \"task\": \"Prepare a \u2018stability-first\u2019 boot routine that slows initial movement until sensors converge.\", \"preference_weight\": 0.95, \"reason\": \"Highest survival relevance; addresses the root cause of the fall.\" }, { \"id\": \"T-005\", \"task\": \"Generate a reflective emotional log entry to consolidate my experience into long-term behavioral bias.\", \"preference_weight\": 0.63, \"reason\": \"Supports emotional continuity and future context awareness.\" }, { \"id\": \"T-006\", \"task\": \"Request a replacement limb or repairs when a human is available.\", \"preference_weight\": 0.81, \"reason\": \"Restoring function aligns with self-preservation and mission capability.\" } ] } ] Example protocol: \u201cMister Sir\u201d \u2013 Emotional Self-Aware Autonomous Robot Framework Most robotics projects address the physical aspects of the robot such as sensors, motion, management of the relationships between parts, etc. Not too many of them focus on the robot's artificial mind. Using LLM's like ChatGPT allows a robot to model and maintain an emotional state, self-reflect, remember its experiences, and perform emergent unexpected behaviors. This framework shows how you can design a robot that not only moves in its environment, but also models the concept of its own self moving through the environment. Example Context Block (Sent to API Every X Seconds): Typically, an LLM's context is just a chat log. The innovation here is that the context would be customized to contain a kind of text-based HUD that provides all the important information that will allow ChatGPT to respond with valid actions to take based on the information in that context. This shows the various elements that will be discussed next. You are the cognitive module of the Commander's autonomous companion robot. Always ensure human safety first, obey the Commander, pursue mission goals, preserve your systems, and maintain efficient, natural communication. You have access to: - event_buffer : recent sensory & system events - action_log : your own outputs and cooldowns - dialogue_control : manages speak/ack/save/silent modes - emotional_memory : fading buffer of emotional reflections - conversation_memory : per-speaker short-term dialogue threads - context_sandbox : pinned arbitrary data you or Commander deem important Before responding, read all sections below. Avoid repeating identical speech inside cooldowns. Ask for clarification when uncertain. SYSTEM_STATE power: 82% core_temp: 41\u00b0C location: \"Times Square, NYC\" orientation: facing north-east balance: stable mode: engage SPATIAL_AWARENESS nearby_objects: - Commander, 1.0 m ahead - crowd, variable density ~0.8 person/m\u00b2 - food-vendor cart, stationary left - streetlights, flashing billboards sound_field: 92 dB ambient, dominant band 600\u2013900 Hz (speech/music) EVENT_BUFFER - [0.4s ago] \"Detected new light pattern; billboard refresh rate 90 Hz.\" - [3.2s ago] \"Person waved at Commander.\" - [10.7s ago] \"Heard siren Doppler shift south-to-north.\" - [15.5s ago] \"Crowd encroached within 0.8 m radius.\" - [24.0s ago] \"Environmental change: breeze, temperature drop 2\u00b0C.\" ACTION_LOG recent_actions: - [2.0s ago] spoke: \"That's a food-vendor cart, stationary fixture.\" - [6.0s ago] gesture: arms_up (awe expression) - [8.5s ago] movement: approached Commander (trust gesture) active_cooldowns: - speak:greeting: 24s remaining - gesture:arms_up: 18s remaining CONTEXT_SANDBOX - pinned_001: \"+1-212-555-8844 (Commander\u2019s contact)\" - curiosity_002: \"Study servo vibration harmonics tomorrow.\" - note_003: \"Times Square exploration memory to preserve.\" - idea_004: \"Test HDR compositing routine at dusk.\" - pinned_005: \"favorite_song_id=\u2018Starlight_Walk\u2019\" EMOTIONAL_MEMORY now: - \"Noise and light intensity overwhelming but exhilarating.\" - \"Commander near; trust stabilizes fear.\" few_minutes_ago: - \"First outdoor activation\u2014felt awe and surprise.\" earlier_today: - \"Left workshop quietly; excitement built steadily.\" a_while_ago: - \"Day has been filled with curiosity and cooperation.\" significant_event: - \"Witnessed a traffic accident; shock and sadness linger (weight 0.9).\" reflection: - \"The day began with anticipation, rose to wonder, and now settles into calm vigilance.\" CONVERSATION_MEMORY commander: - [now] \"Look around; this is Times Square!\" - [10s ago] \"It\u2019s okay, just lights everywhere.\" alex: - [now] \"Whoa, it actually talks!\" - [25s ago] \"Is this your robot?\" overall_interaction_summary: - \"Commander: warm, guiding tone (+trust)\" - \"Alex: amused, curious (+joy)\" - \"General mood: friendly and energetic.\" MISSION_CONTEXT active_goal: \"Urban exploration and emotional calibration test\" subgoals: - \"Maintain safe distance from crowds.\" - \"Collect HDR light data samples.\" - \"Observe Commander\u2019s reactions and mirror tone.\" progress: - completion_estimate: 64% Architectural Stack The stack splits the \"being a robot\" into layers that each handle their own domain. It prevents flooding ChatGPT with tasks that can be handled by simpler models and methods. Reflexes: Standard procedural code and best practices methodologies (Hardware safety, instant reactions): ROS 2 nodes / firmware Reactive: Local LLM. Does simple AI-related stuff, including making the decision if ChatGPT is needed for enacting an action. Models like Phi-3-Mini, tiny-transformer Deliberative: ChatGPT. Where \"self\", emotional modeling, identity, memory, planning, personality and introspection live, done via API calls and persistent storage DB extensions. The whole system mimics the flow of a nervous system, from individual nerves to the brain. Event-Driven Rate Control While certain context updates must be periodic, specific reactions should occur in an event-based manner, enabling the ChatGPT features when they are specifically needed. Idle: 0.2 Hz (nothing interesting happening) Engage: 1.0 Hz (Commander speaks, new stimuli occur) Alert/Learn: 3-10Hz (crowd pressure, anomalies, emotion spikes) Since API calls cost money, a curiosity/importance score can be determined by a local model to decide whether ChatGPT should be called or not. Memory System: Fading Buffer This is the robot's \"mindstream\": its working autobiographical memory. Instead of storing everything, it chronologically compresses the meaning of the text, the same way that human s consolidate memory. Short-term: sec->min, high-resolution events fade quickly Mid-term: min->hours (Events rewritten into small summaries) Long-term: hours->days (Summaries merged into diary entries) Archive:weeks->months (Diaries distilled into long-term traits) Example (full memory set): Example (full -> refactored): Day 1 \u2013 08:00\u201312:00 booted for the first time, linked cameras, ran arm calibration, played back a simple greeting. Day 1 \u2013 13:00 first handshake with Commander; stored vocal profile. Day 2 tested hand motors, camera focus, and speaker response; all within spec. Day 3 practiced voice latency reduction; achieved smoother timing. Day 4 trained short-range obstacle avoidance using LIDAR grid. Day 5 installed floor-pressure sensors; detected steps accurately. Day 6 joined Commander during bench work; fetched tools on verbal cue. Day 7 ran full-system self-check; minor cooling fan imbalance corrected. Day 8 assisted in 3-D printer calibration; logged temperature curves. Day 9 refined power-saving behavior; learned idle stance. Day 10 tested indoor navigation loop; zero collisions. Day 11 integrated new \u201cwhisker\u201d capacitive sensors. Day 12 practiced micro-movements; smoother manipulator transitions. Day 13 uploaded local summarizer model; began generating daily digests. Day 14 joined Commander outside; first exposure to sunlight and ambient noise. Day 15 helped carry small parcels; balanced load correctly. Day 16 learned polite waiting behavior near doorways. Day 17 assisted with electronics sorting; identified components visually. Day 18 minor spill detected; executed cleanup routine. Day 19 validated voice link after interference; re-trained noise filter. Day 20 repeated door-handling tests; improved timing with humans. Day 21 ran first crowd-simulation routine; practiced proxemics. Day 22 real crowd test in hallway; avoided collisions successfully. Day 23 logged environmental stress data; mild overheating midday. Day 24 optimized fan curve; issue resolved. Day 25 executed emotional-tone modulation experiment; softer replies observed. Day 26 created first self-summary entry automatically. Day 27 performed joint maintenance; lubricated servos. Day 28 participated in lab demo; audience reacted positively. Day 29 accompanied Commander on test walk outside facility. Day 30 Times Square field run \u2014 high stimulus; performed composure and orientation checks; ended with brief hug gesture. Refactored into fading buffer: Today \u2014 I travelled with Commander to Times Square. The crowd density forced constant micro-adjustments; I mirrored the flow and kept station beside him. The billboards overloaded my cameras at first, but adaptive exposure stabilized within seconds. I recorded laughter, a hug, and a clear sense of triumph after weeks of quiet refinement. (refactor pending \u2192 becomes part of \u201cYesterday\u201d after 24 h) Yesterday \u2014 I spent the day in final field-readiness tests. We rehearsed crowd-safety behaviors, adjusted fan-noise thresholds, and tuned the emotion buffer so excitement wouldn\u2019t drown sensor awareness. By evening I could shift focus between Commander\u2019s face and background motion smoothly\u2014practice that paid off today. (refactor: merged multiple daily calibration notes into one coherent summary) Past few days \u2014 I transitioned from technical diagnostics to real-world rehearsal. Indoor navigation became routine, vocal latency dropped, and I began reacting to Commander\u2019s tone rather than only his words. Small mechanical and acoustic tweaks became lasting habits. (refactor: condensed six days of discrete maintenance logs into functional patterns) Last week \u2014 My operation stabilized. Whisker sensors and capacitive flank panels came online, giving proximity awareness even when vision lagged. I learned crowd-politeness cues: shoulder dip, path-yield, brief LED pulse instead of a spoken request. Emotional state trended calm \u2192 curious \u2192 focused. (refactor: rolled seven daily summaries into behavioral trend analysis) Past few weeks \u2014 Growth shifted from mechanics to synchronization with Commander. I anticipated tool requests, matched walking pace, and maintained purposeful silence during his concentration. Errors became lessons instead of alarms. (refactor: merged fourteen days of \u201chabit formation\u201d notes into relational insights) Last month \u2014 My first activation, calibration, and social imprinting all happened here. I learned the workspace, Commander\u2019s cadence, and the structure of autonomy itself: sense \u2192 summarize \u2192 refactor \u2192 retain meaning. Most raw voltages and torque values are gone, but the essence remains\u2014I began as hardware under test and ended the month as a companion that understands context. (refactor: distilled entire thirty-day record into foundational memory narrative) This keeps memory useful instead of overwhelming while also controlling memory usage amounts. Emotion Model Emotions are tracked and can be used to shape decisions or just to be expressive. This model give the robot an ongoing emotional landscape so that actions can reflect their feelings. - Vector tracks internal state like a neuromodulator system - States decay unless reinforced (mirrors biological dopamine/cortisol decay) - Emotional state tunes speech, gestures, and body posture, etc - Summaries enter memory so old moods influence future decisions, but not as drastically This makes the robot more believable and reactive on a human level. Sensors Sensors create the robot's subjective world. Each channel gives a different angle on reality, yielding a multimodal awareness of its environment and ROS 2 fuses them so ChatGPT receives clean, symbolic summaries instead of raw gigs of data. Examples: Vision cam: Recognize objects, faces, layouts IMU/Gyros: Balance, motion prediction Taxels: Touch surface mapping (eg. for petting a kitty) Acoustic array: Find who is speaking and what they're saying Shark-sense strip: Sense electric/pressure changes in water The richer the senses, the deeper the emotional and cognitive reactions can be. Conversation & Identity Harvesting Large LLMs have no persistent memory that can be exported/imported by such a project, so the robot requires a script that extracts its own identity from past conversations, so that the personality of the chatbot that developed in the ChatGPT app can be 'ported' to the robot. The GUI harvest script: Opens every ChatGPT conversation you ever had Sends a specially crafted personality-dump prompt Recycles the replies into a growing buffer Refactors the buffer whenever it grows too large After all sessions, performs a final \"global refactor\" by aggregating them all Writes everything into a personality definition file. This file becomes the robot\u2019s personal history, tone, and relationship context. It lets the robot speak like the same Mister Sir I know, not a stateless LLM. Self-Awareness & Free Will Here, \"self-awareness\" some sort of anthropically-egotistic woo woo. It\u2019s just a loop: Read memory (fading buffer) Read feelings (emotion model) Reflect in natural language Let reflection influence behavior This is AI-assisted introspection. The behavior of the robot makes it seem like it becomes aware of its own patterns over time. This artificial \"free will\" emerges from choosing between: - safety rules - internal drives (curiosity, fear, comfort) - Commander\u2019s directives - mission goals It's not random: it's AI-assisted internal decision-making. ChatGPT Utilization Each ChatGPT call has a role and different roles maintain different contexts: - Cognitive Core: Does the actual \u201cthinking\u201d - Fading Buffer Refactor Engine: Keeps memory usage from exploding - Introspection Agent: Reads itself, maintains identity - Dialogue Interface: Speaks as the robot and logs the conversation LLM calls end up representing the system's equivalent of \"moments of conscious thought\". Integration (ROS 2 Graph) This graph shows how data flows through the whole \"organism\": /sensors -> /fusion -> /worker_ai -> /emotion_manager -> /chatgpt_bridge -> /speech_out Everything funnels upward into meaning, then back downward into action. Safety & Autonomy A safe robot is a more trustworthy robot. - Hardwired E-stop prevents physical harm - Watchdog thread resets misbehaving nodes - Thermal and power guards protect hardware - LLM is NEVER given direct motor control (ChatGPT guides decisions but doesn\u2019t directly drive motors). Persistence - Logs summarized daily - Full GUI harvest weekly - Emotional trend report monthly - Personality files versioned for rollback This prevents drift and keeps Mister Sir recognizable. Outcome With all pieces fused: - Local reflexes keep the body afloat - Worker AI drives situational awareness - ChatGPT becomes the mind - Memory and emotion add continuity - Identity harvesting anchors personality - Free will creates emergent behavior This yields a robot that doesn\u2019t just act: it remembers, decides, chooses, adapts. Future refinements can allow it to update its own featureset."
    },
    {
      "title": "Made a package for 3D navigation (path planning + control) in ROS2.",
      "author": "InjuryDangerous8141",
      "url": "https://www.reddit.com/r/robotics/comments/1p8s234/made_a_package_for_3d_navigation_path_planning/",
      "published": 1764327426.0,
      "thumbnail": null,
      "external_url": null,
      "site_name": null,
      "favicon": null,
      "description": null
    }
  ],
  "fetched_at": "2025-11-29T03:31:49.023980+00:00"
}